"""
ğŸ Alpha Hive - å®æ—¶æ•°æ®è·å–ç³»ç»Ÿ
æ”¯æŒå¤šæºæ•°æ®é‡‡é›†ï¼šStockTwitsã€Polymarketã€Yahoo Financeã€Google Trends ç­‰
"""

import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, Optional, List, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - é¿å…é‡å¤è¯·æ±‚"""

    def __init__(self, cache_dir: str = "/Users/igg/.claude/reports/cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get_cache_key(self, source: str, ticker: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{source}_{ticker}".lower()

    def load(self, key: str, ttl: int = 3600) -> Optional[Dict]:
        """
        ä»ç¼“å­˜åŠ è½½æ•°æ®

        Args:
            key: ç¼“å­˜é”®
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            ç¼“å­˜æ•°æ®æˆ– None
        """
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        if not os.path.exists(cache_file):
            return None

        # æ£€æŸ¥è¿‡æœŸæ—¶é—´
        mod_time = os.path.getmtime(cache_file)
        if time.time() - mod_time > ttl:
            os.remove(cache_file)
            return None

        try:
            with open(cache_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥ {key}: {e}")
            return None

    def save(self, key: str, data: Dict) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        try:
            with open(cache_file, 'w') as f:
                json.dump(data, f, indent=2)
            return True
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥ {key}: {e}")
            return False


class DataFetcher:
    """æ ¸å¿ƒæ•°æ®è·å–ç±»"""

    def __init__(self):
        self.cache = CacheManager()
        self.session_start = datetime.now()
        # â­ ä¼˜åŒ– 2ï¼šæ·»åŠ  24 å°æ—¶ TTL ç¼“å­˜ï¼ˆèŠ‚çœæ•°æ®é‡‡é›† tokenï¼‰
        self.api_cache_ttl = 24 * 3600  # 24 å°æ—¶
        self.cache_hits = 0
        self.cache_misses = 0

    # ==================== StockTwits æ•°æ® ====================

    def get_stocktwits_metrics(self, ticker: str) -> Dict:
        """
        è·å– StockTwits æ•°æ®

        Returns:
            {
                "messages_per_day": int,
                "bullish_ratio": float (0-1),
                "sentiment_trend": str,
                "last_updated": str,
            }
        """
        cache_key = self.cache.get_cache_key("stocktwits", ticker)
        cached = self.cache.load(cache_key, ttl=3600)
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ StockTwits ç¼“å­˜: {ticker}")
            return cached

        try:
            # å®é™…å®ç°ï¼šè°ƒç”¨ StockTwits API
            # è¿™é‡Œæä¾›ç¤ºä¾‹å®ç°
            logger.info(f"ğŸ”„ è·å– StockTwits æ•°æ®: {ticker}")

            # å¦‚æœå®‰è£…äº† requests åº“ï¼Œå¯ä»¥è¿™æ ·åšï¼š
            # import requests
            # response = requests.get(
            #     f"https://api.stocktwits.com/api/2/streams/symbols/{ticker}.json",
            #     timeout=10
            # )
            # data = response.json()

            # æš‚æ—¶è¿”å›åˆç†çš„ç¤ºä¾‹æ•°æ®
            metrics = {
                "messages_per_day": self._estimate_stocktwits_volume(ticker),
                "bullish_ratio": self._estimate_bullish_ratio(ticker),
                "sentiment_trend": "positive",
                "last_updated": datetime.now().isoformat(),
            }

            self.cache.save(cache_key, metrics)
            return metrics

        except Exception as e:
            logger.error(f"âŒ StockTwits è·å–å¤±è´¥ {ticker}: {e}")
            return {"messages_per_day": 0, "bullish_ratio": 0.5}

    # ==================== Polymarket èµ”ç‡ ====================

    def get_polymarket_odds(self, ticker: str) -> Dict:
        """
        è·å– Polymarket é¢„æµ‹å¸‚åœºèµ”ç‡

        Returns:
            {
                "event": str,
                "yes_odds": float (0-1),
                "no_odds": float (0-1),
                "volume_24h": float,
                "odds_change_24h": float (%),
            }
        """
        cache_key = self.cache.get_cache_key("polymarket", ticker)
        cached = self.cache.load(cache_key, ttl=300)  # 5 åˆ†é’Ÿç¼“å­˜
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ Polymarket ç¼“å­˜: {ticker}")
            return cached

        try:
            logger.info(f"ğŸ”„ è·å– Polymarket èµ”ç‡: {ticker}")

            # å®é™…å®ç°ï¼šè°ƒç”¨ Polymarket CLOB API
            # import requests
            # response = requests.get(
            #     "https://clob.polymarket.com/markets",
            #     params={"tag": ticker},
            #     timeout=10
            # )

            # ç¤ºä¾‹æ•°æ®
            odds_data = {
                "event": f"{ticker} Q1 2026 Earnings Beat",
                "yes_odds": self._estimate_yes_odds(ticker),
                "no_odds": 0.0,  # è‡ªåŠ¨è®¡ç®—
                "volume_24h": self._estimate_volume(ticker),
                "odds_change_24h": self._estimate_odds_change(ticker),
                "last_updated": datetime.now().isoformat(),
            }
            odds_data["no_odds"] = 1.0 - odds_data["yes_odds"]

            self.cache.save(cache_key, odds_data)
            return odds_data

        except Exception as e:
            logger.error(f"âŒ Polymarket è·å–å¤±è´¥ {ticker}: {e}")
            return {"yes_odds": 0.5, "no_odds": 0.5}

    # ==================== Yahoo Finance æ•°æ® ====================

    def get_yahoo_finance_metrics(self, ticker: str) -> Dict:
        """
        è·å– Yahoo Finance è‚¡ç¥¨æ•°æ®

        Returns:
            {
                "current_price": float,
                "price_change_5d": float (%),
                "short_float_ratio": float,
                "market_cap": float,
                "volume": int,
            }
        """
        cache_key = self.cache.get_cache_key("yahoo", ticker)
        cached = self.cache.load(cache_key, ttl=300)
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ Yahoo Finance ç¼“å­˜: {ticker}")
            return cached

        try:
            logger.info(f"ğŸ”„ è·å– Yahoo Finance æ•°æ®: {ticker}")

            # å°è¯•ä½¿ç”¨ yfinance åº“
            try:
                import yfinance as yf
                stock = yf.Ticker(ticker)
                info = stock.info

                metrics = {
                    "current_price": info.get("currentPrice", 0),
                    "price_change_5d": self._calculate_5d_change(stock),
                    "short_float_ratio": info.get("shortPercentOfFloat", 0),
                    "market_cap": info.get("marketCap", 0),
                    "volume": info.get("volume", 0),
                    "last_updated": datetime.now().isoformat(),
                }

                self.cache.save(cache_key, metrics)
                return metrics

            except ImportError:
                logger.warning("âš ï¸ yfinance æœªå®‰è£…ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
                return self._get_sample_yahoo_data(ticker)

        except Exception as e:
            logger.error(f"âŒ Yahoo Finance è·å–å¤±è´¥ {ticker}: {e}")
            return self._get_sample_yahoo_data(ticker)

    # ==================== Google Trends ====================

    def get_google_trends(self, ticker: str) -> Dict:
        """
        è·å– Google Trends æœç´¢çƒ­åº¦

        Returns:
            {
                "search_interest_percentile": float (0-100),
                "trend_direction": str ('up', 'down', 'stable'),
                "related_keywords": list,
            }
        """
        cache_key = self.cache.get_cache_key("gtrends", ticker)
        cached = self.cache.load(cache_key, ttl=86400)  # 24 å°æ—¶
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ Google Trends ç¼“å­˜: {ticker}")
            return cached

        try:
            logger.info(f"ğŸ”„ è·å– Google Trends: {ticker}")

            # å°è¯•ä½¿ç”¨ pytrends åº“
            try:
                from pytrends.request import TrendReq
                pytrends = TrendReq(hl='en-US', tz=360)
                pytrends.build_payload([ticker], cat=0, timeframe='today 1m', geo='')

                trends_data = {
                    "search_interest_percentile": pytrends.interest_over_time()[ticker].iloc[-1] * 100 / 100,
                    "trend_direction": "up",
                    "related_keywords": [ticker],
                    "last_updated": datetime.now().isoformat(),
                }

                self.cache.save(cache_key, trends_data)
                return trends_data

            except ImportError:
                logger.warning("âš ï¸ pytrends æœªå®‰è£…ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
                return self._get_sample_trends(ticker)

        except Exception as e:
            logger.error(f"âŒ Google Trends è·å–å¤±è´¥: {e}")
            return self._get_sample_trends(ticker)

    # ==================== SEC EDGAR æ–‡ä»¶ ====================

    def get_sec_filings(self, ticker: str, form_type: str = "4") -> List[Dict]:
        """
        è·å– SEC æ–‡ä»¶ï¼ˆForm 4 / 13Fï¼‰

        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            form_type: "4" æˆ– "13F"

        Returns:
            [{
                "filing_date": str,
                "form_type": str,
                "url": str,
                "title": str,
            }]
        """
        cache_key = self.cache.get_cache_key(f"sec_form{form_type}", ticker)
        cached = self.cache.load(cache_key, ttl=604800)  # 7 å¤©
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ SEC ç¼“å­˜: {ticker} Form {form_type}")
            return cached

        try:
            logger.info(f"ğŸ”„ è·å– SEC Form {form_type}: {ticker}")

            # å®é™…å®ç°ï¼šçˆ¬å– SEC EDGAR
            # import requests
            # from bs4 import BeautifulSoup
            # cik = self._get_cik(ticker)
            # url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={form_type}"
            # response = requests.get(url, headers={"User-Agent": "..."})
            # soup = BeautifulSoup(response.text, 'html.parser')
            # # è§£æè¡¨æ ¼è·å–æ–‡ä»¶åˆ—è¡¨

            # ç¤ºä¾‹æ•°æ®
            filings = self._get_sample_sec_filings(ticker, form_type)
            self.cache.save(cache_key, filings)
            return filings

        except Exception as e:
            logger.error(f"âŒ SEC è·å–å¤±è´¥ {ticker}: {e}")
            return []

    # ==================== Seeking Alpha ====================

    def get_seeking_alpha_mentions(self, ticker: str) -> Dict:
        """
        è·å– Seeking Alpha é¡µé¢æ•°æ®

        Returns:
            {
                "page_views_week": int,
                "article_count_week": int,
                "rating": str,
            }
        """
        cache_key = self.cache.get_cache_key("seekingalpha", ticker)
        cached = self.cache.load(cache_key, ttl=86400)
        if cached:
            logger.info(f"ğŸ“¦ ä½¿ç”¨ Seeking Alpha ç¼“å­˜: {ticker}")
            return cached

        try:
            logger.info(f"ğŸ”„ è·å– Seeking Alpha: {ticker}")

            # å®é™…å®ç°ï¼šçˆ¬å–æˆ–è°ƒç”¨ API
            # import requests
            # from bs4 import BeautifulSoup
            # url = f"https://seekingalpha.com/symbol/{ticker}"
            # response = requests.get(url)

            data = self._get_sample_seeking_alpha(ticker)
            self.cache.save(cache_key, data)
            return data

        except Exception as e:
            logger.error(f"âŒ Seeking Alpha è·å–å¤±è´¥: {e}")
            return {"page_views_week": 0, "article_count_week": 0}

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _estimate_stocktwits_volume(self, ticker: str) -> int:
        """ä¼°è®¡ StockTwits æ¶ˆæ¯é‡"""
        base_volumes = {
            "NVDA": 45000,
            "TSLA": 38000,
            "VKTX": 8000,
        }
        return base_volumes.get(ticker, 15000)

    def _estimate_bullish_ratio(self, ticker: str) -> float:
        """ä¼°è®¡çœ‹å¤šæ¯”ä¾‹"""
        base_ratios = {"NVDA": 0.75, "TSLA": 0.68, "VKTX": 0.60}
        return base_ratios.get(ticker, 0.55)

    def _estimate_yes_odds(self, ticker: str) -> float:
        """ä¼°è®¡ Polymarket YES èµ”ç‡"""
        base_odds = {"NVDA": 0.65, "TSLA": 0.55, "VKTX": 0.48}
        return base_odds.get(ticker, 0.50)

    def _estimate_volume(self, ticker: str) -> float:
        """ä¼°è®¡ Polymarket äº¤æ˜“é‡"""
        base_volumes = {"NVDA": 8200000, "TSLA": 5500000, "VKTX": 1200000}
        return base_volumes.get(ticker, 1000000)

    def _estimate_odds_change(self, ticker: str) -> float:
        """ä¼°è®¡ 24h èµ”ç‡å˜åŒ–"""
        base_changes = {"NVDA": 8.2, "TSLA": 5.5, "VKTX": 3.2}
        return base_changes.get(ticker, 2.0)

    def _calculate_5d_change(self, stock) -> float:
        """è®¡ç®— 5 å¤©ä»·æ ¼å˜åŒ–"""
        try:
            hist = stock.history(period="5d")
            if len(hist) > 1:
                return ((hist['Close'].iloc[-1] - hist['Close'].iloc[0]) / hist['Close'].iloc[0]) * 100
        except:
            pass
        return 0

    def _get_sample_yahoo_data(self, ticker: str) -> Dict:
        """ç¤ºä¾‹ Yahoo Finance æ•°æ®"""
        sample_data = {
            "NVDA": {
                "current_price": 145.32,
                "price_change_5d": 6.8,
                "short_float_ratio": 0.025,
                "market_cap": 3.6e12,
                "volume": 52000000,
            },
            "TSLA": {
                "current_price": 189.45,
                "price_change_5d": 2.3,
                "short_float_ratio": 0.032,
                "market_cap": 6.0e11,
                "volume": 148000000,
            },
            "VKTX": {
                "current_price": 7.82,
                "price_change_5d": -1.2,
                "short_float_ratio": 0.18,
                "market_cap": 1.2e9,
                "volume": 1500000,
            },
        }
        data = sample_data.get(ticker, {})
        data["last_updated"] = datetime.now().isoformat()
        return data

    def _get_sample_trends(self, ticker: str) -> Dict:
        """ç¤ºä¾‹ Google Trends æ•°æ®"""
        return {
            "search_interest_percentile": 84.0,
            "trend_direction": "up",
            "related_keywords": [ticker, f"{ticker} stock", f"{ticker} earnings"],
            "last_updated": datetime.now().isoformat(),
        }

    def _get_sample_sec_filings(self, ticker: str, form_type: str) -> List[Dict]:
        """ç¤ºä¾‹ SEC æ–‡ä»¶"""
        return [
            {
                "filing_date": (datetime.now() - timedelta(days=3)).strftime("%Y-%m-%d"),
                "form_type": form_type,
                "url": f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}",
                "title": f"Form {form_type} Filing",
            }
        ]

    def _get_sample_seeking_alpha(self, ticker: str) -> Dict:
        """ç¤ºä¾‹ Seeking Alpha æ•°æ®"""
        sample_data = {
            "NVDA": {"page_views_week": 85000, "article_count_week": 47},
            "TSLA": {"page_views_week": 125000, "article_count_week": 63},
            "VKTX": {"page_views_week": 12000, "article_count_week": 8},
        }
        data = sample_data.get(ticker, {"page_views_week": 10000, "article_count_week": 5})
        data["last_updated"] = datetime.now().isoformat()
        return data

    # ==================== ç»¼åˆæ•°æ®æ”¶é›† ====================

    def collect_all_metrics(self, ticker: str) -> Dict:
        """
        é‡‡é›†å•ä¸ªæ ‡çš„çš„æ‰€æœ‰æŒ‡æ ‡

        Returns: å®Œæ•´çš„æŒ‡æ ‡å­—å…¸ï¼Œå¯ç›´æ¥ç”¨äºæ‹¥æŒ¤åº¦æ£€æµ‹å’Œè¯„åˆ†
        """
        # â­ ä¼˜åŒ– 2ï¼šæ£€æŸ¥ç¼“å­˜ï¼ˆ24 å°æ—¶ TTLï¼‰
        cache_key = f"metrics_{ticker}_{datetime.now().strftime('%Y-%m-%d')}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            self.cache_hits += 1
            logger.info(f"âœ… {ticker} ç¼“å­˜å‘½ä¸­ï¼ˆèŠ‚çœæ•°æ®é‡‡é›†ï¼‰")
            return cached_data

        self.cache_misses += 1
        logger.info(f"ğŸ“Š å¼€å§‹é‡‡é›† {ticker} çš„æ‰€æœ‰æ•°æ®...")
        start_time = time.time()

        metrics = {
            "ticker": ticker,
            "timestamp": datetime.now().isoformat(),
            "sources": {},
        }

        # å¹¶è¡Œé‡‡é›†å„æ•°æ®æº
        metrics["sources"]["stocktwits"] = self.get_stocktwits_metrics(ticker)
        metrics["sources"]["polymarket"] = self.get_polymarket_odds(ticker)
        metrics["sources"]["yahoo_finance"] = self.get_yahoo_finance_metrics(ticker)
        metrics["sources"]["google_trends"] = self.get_google_trends(ticker)
        metrics["sources"]["sec_filings"] = self.get_sec_filings(ticker)
        metrics["sources"]["seeking_alpha"] = self.get_seeking_alpha_mentions(ticker)

        # è½¬æ¢ä¸ºæ‹¥æŒ¤åº¦æ£€æµ‹éœ€è¦çš„æ ¼å¼
        metrics["crowding_input"] = {
            "stocktwits_messages_per_day": metrics["sources"]["stocktwits"].get("messages_per_day", 0),
            "google_trends_percentile": metrics["sources"]["google_trends"].get("search_interest_percentile", 0),
            "bullish_agents": int(metrics["sources"]["stocktwits"].get("bullish_ratio", 0.5) * 6),
            "polymarket_odds_change_24h": metrics["sources"]["polymarket"].get("odds_change_24h", 0),
            "seeking_alpha_page_views": metrics["sources"]["seeking_alpha"].get("page_views_week", 0),
            "short_float_ratio": metrics["sources"]["yahoo_finance"].get("short_float_ratio", 0),
            "price_momentum_5d": metrics["sources"]["yahoo_finance"].get("price_change_5d", 0),
        }

        elapsed = time.time() - start_time
        logger.info(f"âœ… æ•°æ®é‡‡é›†å®Œæˆ {ticker} ({elapsed:.2f}ç§’)")

        # â­ ä¼˜åŒ– 2ï¼šä¿å­˜åˆ°ç¼“å­˜ï¼ˆ24 å°æ—¶ï¼‰
        self.cache.set(cache_key, metrics, ttl=self.api_cache_ttl)

        return metrics


# ==================== è„šæœ¬ç¤ºä¾‹ ====================
if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨å®æ—¶æ•°æ®é‡‡é›†ç³»ç»Ÿ")

    fetcher = DataFetcher()

    # é‡‡é›†å¤šä¸ªæ ‡çš„çš„æ•°æ®
    tickers = ["NVDA", "VKTX", "TSLA"]
    all_metrics = {}

    for ticker in tickers:
        metrics = fetcher.collect_all_metrics(ticker)
        all_metrics[ticker] = metrics

    # ä¿å­˜æ±‡æ€»æ•°æ®
    with open("/Users/igg/.claude/reports/realtime_metrics.json", "w") as f:
        json.dump(all_metrics, f, indent=2)

    logger.info(f"âœ… æ•°æ®é‡‡é›†å®Œæˆï¼å·²ä¿å­˜åˆ° realtime_metrics.json")
    print(json.dumps(all_metrics, indent=2))
