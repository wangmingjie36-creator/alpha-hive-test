# ğŸš€ Alpha Hive å®æ—¶æ•°æ®é›†æˆæŒ‡å—

> **æ—¥æœŸ**ï¼š2026-02-23
> **çŠ¶æ€**ï¼šâœ… å®æ—¶æ•°æ®ç³»ç»Ÿå·²é…ç½®
> **ç‰ˆæœ¬**ï¼š1.0

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [å®‰è£…ä¾èµ–](#å®‰è£…ä¾èµ–)
3. [æ•°æ®æºé…ç½®](#æ•°æ®æºé…ç½®)
4. [è¿è¡Œå®æ—¶é‡‡é›†](#è¿è¡Œå®æ—¶é‡‡é›†)
5. [é›†æˆåˆ°æŠ¥å‘Š](#é›†æˆåˆ°æŠ¥å‘Š)
6. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
7. [æˆæœ¬åˆ†æ](#æˆæœ¬åˆ†æ)

---

## å¿«é€Ÿå¼€å§‹

### ç¬¬ 1 æ­¥ï¼šéªŒè¯åŸºç¡€è®¾ç½®

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
python3 --version  # éœ€è¦ 3.8+

# æ£€æŸ¥å·²æœ‰çš„æ–‡ä»¶
ls -lh config.py data_fetcher.py
```

### ç¬¬ 2 æ­¥ï¼šå®‰è£…ä¾èµ–åº“

```bash
# å®‰è£…æ‰€éœ€çš„ Python åŒ…
pip3 install requests yfinance pytrends beautifulsoup4

# å¯é€‰ï¼šç”¨äºæ•°æ®å¤„ç†
pip3 install pandas numpy

# å¯é€‰ï¼šç”¨äºå®šæ—¶ä»»åŠ¡
pip3 install schedule APScheduler
```

### ç¬¬ 3 æ­¥ï¼šè¿è¡Œé¦–æ¬¡é‡‡é›†

```bash
# æ‰§è¡Œæ•°æ®é‡‡é›†è„šæœ¬
python3 data_fetcher.py

# æ£€æŸ¥è¾“å‡º
cat realtime_metrics.json | jq '.' | head -50
```

---

## å®‰è£…ä¾èµ–

### å¿…éœ€çš„åº“

| åº“ | ç”¨é€” | å®‰è£… | è¯´æ˜ |
|----|----|------|------|
| `requests` | HTTP è¯·æ±‚ | `pip install requests` | è°ƒç”¨ API |
| `yfinance` | Yahoo Finance | `pip install yfinance` | è‚¡ç¥¨ä»·æ ¼ã€åšç©ºæ¯”ä¾‹ |
| `pytrends` | Google Trends | `pip install pytrends` | æœç´¢çƒ­åº¦æ•°æ® |
| `beautifulsoup4` | ç½‘é¡µçˆ¬å– | `pip install beautifulsoup4` | è§£æ HTML |

### å¯é€‰çš„åº“

| åº“ | ç”¨é€” | å®‰è£… |
|----|------|------|
| `schedule` | å®šæ—¶ä»»åŠ¡ | `pip install schedule` |
| `APScheduler` | é«˜çº§å®šæ—¶ | `pip install APScheduler` |
| `pandas` | æ•°æ®å¤„ç† | `pip install pandas` |

### å®Œæ•´å®‰è£…

```bash
# ä¸€æ¬¡æ€§å®‰è£…æ‰€æœ‰åº“
pip3 install requests yfinance pytrends beautifulsoup4 schedule APScheduler pandas

# éªŒè¯å®‰è£…
python3 -c "import yfinance; import requests; print('âœ… ä¾èµ–å®‰è£…æˆåŠŸ')"
```

---

## æ•°æ®æºé…ç½®

### 1ï¸âƒ£ StockTwits API

**æ— éœ€è®¤è¯** âœ…ï¼ˆå…¬å¼€æ•°æ®ï¼‰

```python
# data_fetcher.py ä¸­å·²é›†æˆ
# ç›´æ¥è°ƒç”¨å³å¯
fetcher.get_stocktwits_metrics("NVDA")
```

### 2ï¸âƒ£ Polymarket API

**æ— éœ€è®¤è¯** âœ…ï¼ˆå…¬å¼€æ•°æ®ï¼‰

```python
# å®Œå…¨å…è´¹çš„é¢„æµ‹å¸‚åœºæ•°æ®
fetcher.get_polymarket_odds("NVDA")
```

### 3ï¸âƒ£ Yahoo Finance

**æ— éœ€è®¤è¯** âœ…ï¼ˆyfinance åº“å¤„ç†ï¼‰

```python
# é€šè¿‡ yfinance åº“è‡ªåŠ¨å¤„ç†è®¤è¯
fetcher.get_yahoo_finance_metrics("NVDA")
```

### 4ï¸âƒ£ Google Trends

**æ— éœ€è®¤è¯** âœ…ï¼ˆpytrends åº“å¤„ç†ï¼‰

```bash
# æ³¨æ„ï¼špytrends æœ‰åçˆ¬è™«é™åˆ¶ï¼Œå»ºè®®ä½¿ç”¨ç¼“å­˜
pip install pytrends
```

**ä½¿ç”¨å»ºè®®**ï¼š
- ç¼“å­˜æ—¶é—´ï¼š24 å°æ—¶
- æŸ¥è¯¢é—´éš”ï¼šé¿å…è¿ç»­è¯·æ±‚
- å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ Google å®˜æ–¹ APIï¼ˆéœ€ä»˜è´¹ï¼‰

### 5ï¸âƒ£ SEC EDGARï¼ˆå¯é€‰ï¼‰

**æ— éœ€è®¤è¯** âœ…ï¼ˆéœ€è¦ç½‘é¡µçˆ¬å–ï¼‰

```bash
# å®‰è£…çˆ¬å–åº“
pip install beautifulsoup4 selenium

# è·å– CIK å·ï¼ˆä¸€æ¬¡æ€§ï¼‰
python3
>>> from data_fetcher import DataFetcher
>>> fetcher = DataFetcher()
>>> fetcher.get_sec_filings("NVDA", form_type="4")
```

**æ³¨æ„**ï¼š
- ä»…ç”¨äºä¸»è¦æŠ•èµ„äººæŠ«éœ²ï¼ˆForm 4ï¼‰
- å»ºè®®ç¼“å­˜ 7 å¤©ä»¥ä¸Š
- SEC æœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶

### 6ï¸âƒ£ Seeking Alphaï¼ˆå¯é€‰ï¼‰

**éƒ¨åˆ†åŠŸèƒ½éœ€ä»˜è´¹** âš ï¸

```bash
# ä½¿ç”¨ç½‘é¡µçˆ¬å–ï¼ˆéœ€ä»£ç†é¿å…è¢«å°ï¼‰
pip install selenium cloudscraper

# æˆ–ä½¿ç”¨å®˜æ–¹ APIï¼ˆéœ€è®¢é˜…ï¼‰
# è¯¦è§ï¼šhttps://api.seekingalpha.com
```

---

## è¿è¡Œå®æ—¶é‡‡é›†

### æ–¹å¼ 1ï¼šå•æ¬¡é‡‡é›†

```bash
# é‡‡é›†å•ä¸ªæ ‡çš„
python3 -c "
from data_fetcher import DataFetcher
fetcher = DataFetcher()
metrics = fetcher.collect_all_metrics('NVDA')
print(metrics)
"
```

### æ–¹å¼ 2ï¼šæ‰¹é‡é‡‡é›†

```bash
# è¿è¡Œå®Œæ•´è„šæœ¬ï¼ˆé‡‡é›† NVDAã€VKTXã€TSLAï¼‰
python3 data_fetcher.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```json
{
  "NVDA": {
    "timestamp": "2026-02-23T10:30:45.123456",
    "sources": {
      "stocktwits": {
        "messages_per_day": 45000,
        "bullish_ratio": 0.75,
        "sentiment_trend": "positive"
      },
      "polymarket": {
        "yes_odds": 0.65,
        "volume_24h": 8200000,
        "odds_change_24h": 8.2
      },
      "yahoo_finance": {
        "current_price": 145.32,
        "price_change_5d": 6.8,
        "short_float_ratio": 0.025
      }
    },
    "crowding_input": {
      "stocktwits_messages_per_day": 45000,
      "google_trends_percentile": 84.0,
      "bullish_agents": 5,
      "polymarket_odds_change_24h": 8.2
    }
  }
}
```

### æ–¹å¼ 3ï¼šå®šæ—¶é‡‡é›†ï¼ˆæ¨èï¼‰

**ä½¿ç”¨ schedule åº“**ï¼š

```python
# scheduler.py
import schedule
import time
from data_fetcher import DataFetcher

def collect_and_report():
    fetcher = DataFetcher()
    tickers = ["NVDA", "VKTX", "TSLA"]
    for ticker in tickers:
        metrics = fetcher.collect_all_metrics(ticker)
        print(f"âœ… {ticker} æ•°æ®å·²æ›´æ–°")

# æ¯ 5 åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡ï¼ˆPolymarket å¿«é€Ÿå˜åŒ–ï¼‰
schedule.every(5).minutes.do(collect_and_report)

# æ¯ 1 å°æ—¶é‡‡é›†ä¸€æ¬¡ï¼ˆStockTwitsï¼‰
schedule.every(1).hours.do(collect_and_report)

while True:
    schedule.run_pending()
    time.sleep(1)
```

**è¿è¡Œå®šæ—¶é‡‡é›†**ï¼š

```bash
python3 scheduler.py &
```

**ä½¿ç”¨ APSchedulerï¼ˆæ›´å¼ºå¤§ï¼‰**ï¼š

```python
# scheduler_advanced.py
from apscheduler.schedulers.background import BackgroundScheduler
from data_fetcher import DataFetcher
import logging

scheduler = BackgroundScheduler()
fetcher = DataFetcher()

def collect_job():
    for ticker in ["NVDA", "VKTX", "TSLA"]:
        fetcher.collect_all_metrics(ticker)

# æ¯ 5 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡
scheduler.add_job(collect_job, 'interval', minutes=5)
scheduler.start()

print("âœ… å®šæ—¶é‡‡é›†å·²å¯åŠ¨")
```

---

## é›†æˆåˆ°æŠ¥å‘Š

### æ›´æ–° generate_optimized_report.py

ä¿®æ”¹ç”ŸæˆæŠ¥å‘Šè„šæœ¬ï¼Œä½¿ç”¨å®æ—¶æ•°æ®ï¼š

```python
from data_fetcher import DataFetcher
from crowding_detector import CrowdingDetector

def generate_realtime_report(ticker: str) -> str:
    """ä½¿ç”¨å®æ—¶æ•°æ®ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""

    # 1. é‡‡é›†å®æ—¶æ•°æ®
    fetcher = DataFetcher()
    metrics = fetcher.collect_all_metrics(ticker)

    # 2. æ‹¥æŒ¤åº¦æ£€æµ‹ï¼ˆä½¿ç”¨å®æ—¶æ•°æ®ï¼‰
    detector = CrowdingDetector(ticker)
    crowding_score, scores = detector.calculate_crowding_score(
        metrics["crowding_input"]
    )

    # 3. ç”ŸæˆæŠ¥å‘Š
    html = generate_html_with_realtime_data(
        ticker=ticker,
        metrics=metrics,
        crowding_score=crowding_score,
        scores=scores
    )

    return html

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    for ticker in ["NVDA", "VKTX", "TSLA"]:
        html = generate_realtime_report(ticker)
        with open(f"alpha-hive-{ticker}-realtime.html", "w") as f:
            f.write(html)
```

### å®Œæ•´é›†æˆè„šæœ¬

```bash
#!/bin/bash
# realtime_report_generator.sh

# é‡‡é›†æ•°æ®
python3 data_fetcher.py

# ç”ŸæˆæŠ¥å‘Š
python3 generate_optimized_report.py --realtime

# ä¸Šä¼ åˆ° GitHub Pages
git add alpha-hive-*.html realtime_metrics.json
git commit -m "ğŸ”„ å®æ—¶æŠ¥å‘Šæ›´æ–° - $(date '+%Y-%m-%d %H:%M:%S')"
git push origin main

echo "âœ… å®æ—¶æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¸Šä¼ "
```

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šImportError - æ‰¾ä¸åˆ°æ¨¡å—

```bash
# ç—‡çŠ¶ï¼šModuleNotFoundError: No module named 'yfinance'

# è§£å†³ï¼š
pip3 install yfinance --upgrade

# éªŒè¯ï¼š
python3 -c "import yfinance; print('âœ… OK')"
```

### é—®é¢˜ 2ï¼šç½‘ç»œè¶…æ—¶

```bash
# ç—‡çŠ¶ï¼šrequests.exceptions.Timeout

# è§£å†³æ–¹æ¡ˆ 1ï¼šå¢åŠ è¶…æ—¶æ—¶é—´
# åœ¨ data_fetcher.py ä¸­ä¿®æ”¹ï¼š
TIMEOUT = 20  # ä» 10 æ”¹ä¸º 20 ç§’

# è§£å†³æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ä»£ç†
PROXIES = {
    "http": "http://proxy.example.com:8080",
    "https": "http://proxy.example.com:8080",
}

# è§£å†³æ–¹æ¡ˆ 3ï¼šæ£€æŸ¥ç½‘ç»œ
curl -I https://api.stocktwits.com
```

### é—®é¢˜ 3ï¼šæ•°æ®æºæ— å“åº”

```bash
# ç—‡çŠ¶ï¼šé‡‡é›†æŸä¸ªæ•°æ®æºå¤±è´¥

# æ£€æŸ¥æ—¥å¿—ï¼š
cat /Users/igg/.claude/reports/logs/data_fetcher.log

# éªŒè¯ API å¯ç”¨æ€§ï¼š
python3 -c "
import requests
response = requests.get('https://api.stocktwits.com/api/2/streams/symbols/NVDA.json', timeout=5)
print(response.status_code)
"
```

### é—®é¢˜ 4ï¼šç¼“å­˜å¯¼è‡´æ•°æ®è¿‡æœŸ

```bash
# ç—‡çŠ¶ï¼šæ•°æ®ä¸æ›´æ–°

# æ¸…é™¤ç¼“å­˜ï¼š
rm -rf /Users/igg/.claude/reports/cache/*

# æˆ–ä¿®æ”¹ TTLï¼š
# åœ¨ config.py ä¸­è°ƒæ•´ç¼“å­˜è¿‡æœŸæ—¶é—´
CACHE_CONFIG = {
    "ttl": {
        "stocktwits": 1800,  # 30 åˆ†é’Ÿè€Œä¸æ˜¯ 1 å°æ—¶
        "polymarket": 60,    # 1 åˆ†é’Ÿè€Œä¸æ˜¯ 5 åˆ†é’Ÿ
    }
}
```

### é—®é¢˜ 5ï¼šå†…å­˜ä¸è¶³

```bash
# ç—‡çŠ¶ï¼šMemoryError

# è§£å†³ï¼šä½¿ç”¨æµå¼å¤„ç†
# ä¿®æ”¹ data_fetcher.pyï¼š

def collect_metrics_streaming(self, tickers: List[str]):
    """æµå¼é‡‡é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®"""
    for ticker in tickers:
        yield self.collect_all_metrics(ticker)
```

---

## æˆæœ¬åˆ†æ

### å…è´¹æ•°æ®æº âœ…

| æ•°æ®æº | é¢åº¦ | æˆæœ¬ |
|--------|------|------|
| StockTwits | æ— é™ | å…è´¹ |
| Polymarket | æ— é™ | å…è´¹ |
| Yahoo Finance | æ— é™ | å…è´¹ |
| Google Trends | ~200 req/day | å…è´¹ |
| SEC EDGAR | æ— é™ | å…è´¹ |

**æ€»æˆæœ¬**ï¼š$0/æœˆ âœ…

### å¯é€‰ä»˜è´¹å‡çº§

| æœåŠ¡ | åŠŸèƒ½ | ä»·æ ¼ |
|------|------|------|
| Alpha Vantage API | å®æ—¶è‚¡ç¥¨æ•°æ® | $5-500/æœˆ |
| Seeking Alpha Premium | é«˜çº§ç ”ç©¶ | $239/å¹´ |
| Bloomberg Terminal | ä¼ä¸šçº§æ•°æ® | $24,000/å¹´ |
| IQFeed | æœŸæƒæ•°æ® | $148/æœˆ |

**å»ºè®®**ï¼šä½¿ç”¨å…è´¹æ•°æ®æºé¦–å…ˆéªŒè¯ç³»ç»Ÿï¼Œå†è€ƒè™‘ä»˜è´¹å‡çº§ã€‚

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç¼“å­˜ç­–ç•¥

```python
# æ ¹æ®æ•°æ®å˜åŒ–é¢‘ç‡è®¾ç½®ä¸åŒçš„ TTL
CACHE_TTL = {
    "polymarket": 300,      # 5 åˆ†é’Ÿï¼ˆå¿«é€Ÿå˜åŒ–ï¼‰
    "stocktwits": 3600,     # 1 å°æ—¶
    "google_trends": 86400, # 24 å°æ—¶
    "sec_filings": 604800,  # 7 å¤©
}
```

### 2. é”™è¯¯å¤„ç†

```python
# å®ç°ä¼˜é›…é™çº§
try:
    data = fetcher.get_polymarket_odds(ticker)
except ConnectionError:
    logger.warning(f"Polymarket ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
    data = cache.load(key, ttl=0)  # å¿½ç•¥ TTL
```

### 3. é€Ÿç‡é™åˆ¶

```python
# é¿å…è¢« API é™æµ
import time
time.sleep(1)  # è¯·æ±‚é—´å»¶è¿Ÿ 1 ç§’
```

### 4. ç›‘æ§ä¸å‘Šè­¦

```python
# å®šæœŸæ£€æŸ¥æ•°æ®è´¨é‡
def health_check():
    for ticker in WATCHLIST:
        metrics = fetcher.collect_all_metrics(ticker)
        if metrics is None:
            send_alert(f"âš ï¸ {ticker} æ•°æ®é‡‡é›†å¤±è´¥")
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **éªŒè¯æ•°æ®è´¨é‡**
   ```bash
   python3 data_fetcher.py
   cat realtime_metrics.json | jq '.NVDA.crowding_input'
   ```

2. **é›†æˆåˆ°æŠ¥å‘Šç”Ÿæˆ**
   ```bash
   # æ›´æ–° generate_optimized_report.py
   # ä½¿ç”¨ realtime_metrics.json è€Œä¸æ˜¯ç¡¬ç¼–ç æ•°æ®
   ```

3. **è®¾ç½®å®šæ—¶é‡‡é›†**
   ```bash
   # åˆ›å»º cron ä»»åŠ¡æˆ–ä½¿ç”¨ scheduler.py
   */5 * * * * python3 /path/to/data_fetcher.py
   ```

4. **ç›‘æ§æ•°æ®æº**
   ```bash
   # å®šæœŸæ£€æŸ¥æ•°æ®è´¨é‡
   python3 -c "from data_fetcher import DataFetcher; f = DataFetcher(); print(f.collect_all_metrics('NVDA'))"
   ```

---

## ğŸ“ å¸¸è§é—®é¢˜

**Q: æ˜¯å¦å¯ä»¥è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šï¼Ÿ**
A: æ˜¯çš„ã€‚ä½¿ç”¨ scheduler.py + generate_optimized_report.py çš„ç»„åˆï¼Œå¯ä»¥æ¯ 5 åˆ†é’Ÿè‡ªåŠ¨æ›´æ–°ä¸€æ¬¡ã€‚

**Q: æ•°æ®å»¶è¿Ÿæœ‰å¤šä¹…ï¼Ÿ**
A:
- Polymarketï¼šå®æ—¶ï¼ˆ<1 ç§’ï¼‰
- StockTwitsï¼š5-10 åˆ†é’Ÿ
- Google Trendsï¼šæ•°å°æ—¶å»¶è¿Ÿ
- SEC EDGARï¼š1-2 å¤©å»¶è¿Ÿ

**Q: å¦‚ä½•å¤„ç†æ•°æ®ç¼ºå¤±ï¼Ÿ**
A: data_fetcher.py ä¼šè‡ªåŠ¨é™çº§åˆ°ç¤ºä¾‹æ•°æ®ï¼ŒåŒæ—¶è®°å½•é”™è¯¯æ—¥å¿—ä¾›æ’æŸ¥ã€‚

**Q: å¯å¦ç¦»çº¿è¿è¡Œï¼Ÿ**
A: ä¸è¡Œã€‚å®æ—¶æ•°æ®é‡‡é›†éœ€è¦äº’è”ç½‘è¿æ¥ã€‚å¯ä»¥å…ˆé‡‡é›†æ•°æ®ä¿å­˜åˆ° JSONï¼Œç„¶åç¦»çº¿ä½¿ç”¨ã€‚

---

**æœ€åæ›´æ–°**ï¼š2026-02-23
**ç»´æŠ¤è€…**ï¼šAlpha Hive Team
**åé¦ˆ**ï¼šæäº¤ Issue æˆ– PR
